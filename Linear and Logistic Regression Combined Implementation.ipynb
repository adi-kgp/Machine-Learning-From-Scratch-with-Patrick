{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNolJ7DVrzS2/94Ilj5JGkH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Linear and Logistic Regression Combined"],"metadata":{"id":"Ih43d9-fZv3N"}},{"cell_type":"markdown","source":["### Refactoring code from two previous classes LinearRegression and LogisticRegression by creating a common base class"],"metadata":{"id":"C1gO59RXad0s"}},{"cell_type":"code","source":["import numpy as np\n","\n","class BaseRegression:\n","\n","    def __init__(self, lr=0.001, n_iters=1000):\n","        self.lr = lr\n","        self.n_iters = n_iters\n","        self.weights = None\n","        self.bias = None\n","\n","    def fit(self, X, y):\n","        # init our parameters\n","        n_samples, n_features = X.shape\n","        self.weights = np.zeros(n_features)\n","        self.bias = 0\n","        # gradient descent\n","        for _ in range(self.n_iters):\n","            y_predicted = np.dot(X, self.weights) + self.bias\n","\n","            # compute gradients\n","            dw = (1/n_samples) * np.dot(X.T, (y_predicted - y))\n","            db = (1/n_samples) * np.sum(y_predicted - y)\n","\n","            # update parameters\n","            self.weights -= self.lr * dw\n","            self.bias -= self.lr * db\n","\n","    def predict(self, X):\n","        return self._predict(X, self.weights, self.bias)\n","\n","    def _approximation(self, X, w, b):\n","        raise NotImplementedError()\n","\n","    def _predict(self, X, w, b):\n","        raise NotImplementedError()\n","\n","class LinearRegression(BaseRegression):\n","\n","    def _approximation(self, X, w, b):\n","        return np.dot(X, w) + b\n","\n","    def _predict(self, X, w, b):\n","        return np.dot(X, w) + b\n","\n","class LogisticRegression(BaseRegression):\n","\n","    def _approximation(self, X, w, b):\n","        linear_model = np.dot(X, w) + b\n","        return self._sigmoid(linear_model)\n","\n","    def _predict(self, X, w, b):\n","        linear_model = np.dot(X, w) + b\n","        y_predicted = self._sigmoid(linear_model)\n","        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n","        return y_predicted_cls\n","\n","    def _sigmoid(self, x):\n","        return 1 / (1 + np.exp(-x))"],"metadata":{"id":"oc6CDqq1bRyO","executionInfo":{"status":"ok","timestamp":1672562412512,"user_tz":-330,"elapsed":620,"user":{"displayName":"Aditya Sharma","userId":"18274581938771633663"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","import matplotlib.pyplot as plt\n","\n","X, y = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=4)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n","\n","regressor = LinearRegression(lr=0.01)\n","regressor.fit(X_train, y_train)\n","predicted = regressor.predict(X_test)\n","\n","def mse(y_true, y_predict):\n","    return np.mean((y_true - predicted)**2)\n","\n","mse_value = mse(y_test, predicted)\n","print(mse_value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H7b78pLJaVok","executionInfo":{"status":"ok","timestamp":1672564825530,"user_tz":-330,"elapsed":465,"user":{"displayName":"Aditya Sharma","userId":"18274581938771633663"}},"outputId":"27595697-8b27-4659-8258-b27f03056d35"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["305.7719958301902\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"QbQGNovGfQ5L"},"execution_count":null,"outputs":[]}]}